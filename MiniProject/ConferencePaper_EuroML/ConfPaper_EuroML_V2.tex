\documentclass[10pt, conference, a4paper, final]{IEEEtran}
\IEEEoverridecommandlockouts
% For better handling of math expressions
\usepackage{amsmath}

\usepackage[margin=1in]{geometry} % Adjust margins as needed
\usepackage{lipsum} % Provides sample text. Remove this for your actual document.
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{booktabs}

% For better formatting of lists
\usepackage{enumitem}
\usepackage{amssymb}
% Optional for improved typography
\usepackage{microtype}
\usepackage[margin=1in]{geometry} % Adjust margins as needed
\usepackage{lipsum} % Provides sample text. Remove this for your actual document.
\usepackage[table,xcdraw]{xcolor}
\usepackage{colortbl}
\usepackage{graphicx} % Required for including images
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{relsize, fullpage, url, array}
\usepackage{lscape, afterpage}
\usepackage{float,lscape}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{tabularx}

\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}

\title{Critical Pixels:Strengthen Deep Models Against Adversarial Attacks with SHAP Gradient }
\author{Arooj Arif}
\date{\today} % You can also specify a date manually

\begin{document}

\maketitle % This command creates the title


\begin{abstract}    
    Adversarial perturbations pose a significant challenge to the reliability and transparency of deep learning models. These perturbations are usually invisible to human observers, requiring the development of sophisticated defense strategies. In our research, we looked closely at four different adversarial attacks. We carefully studied how these attacks impact the vulnerability of deep learning models, especially when considering different perturbation sizes. To investigate these attacks further, we used an extension of SHAP (SHapley Additive exPlanations), Gradient Explainer. With the help of SHAP signatures, we examine how attackers manipulate specific pixels in an image to confuse the model. We also figured out which features or characteristics of the data tend to impact the model's predictions positively and which have a negative impact. Our work has made deep learning models more robust and understandable. This is particularly important in critical areas like healthcare, aerospace, and self-driving cars, where having robust and transparent models is essential for safety and reliability. Our research highlights the importance of integrating robustness and interpretability in deep learning models to enhance their safety and protect them against the dynamic spectrum of security threats in the digital age. Our contributions signify a noteworthy enhancement in the defense mechanisms of deep learning models.


\end{abstract}

\section{Introduction}

Deep learning is becoming increasingly important and is now a vital component of modern computational applications. It uses multiple layers to process data and create computational models, introducing various algorithms such as generative adversarial networks, convolutional neural networks, and model transfers. These algorithms have revolutionized the way we process information and have shown significant progress in domains such as visual, audio, and text processing, social network analysis, and natural language processing. It has also successfully addressed challenges in machine learning such as unsupervised and online learning. Its ability to handle massive and complex datasets has made it a critical tool for extensive data analysis. Compared to traditional machine learning approaches, its state-of-the-art performance has led to its widespread adoption in fields such as image processing, computer vision, speech recognition, machine translation, medical imaging, and many others.\cite {Saad, Md}. In today's world, with the rapid advancements and widespread adoption of technology, it has become crucial to focus on the strength and reliability of deep learning models. These models are increasingly being used in critical safety and socially significant applications like autonomous driving, face recognition, and malware detection, where their efficiency and dependability are of utmost importance. However, deep neural networks have been found to be susceptible to both adversarial and natural image detection that can significantly impact their performance. By exploring and improving the robustness of these models, we can ensure their effectiveness and reliability in real-world scenarios \cite {Numair, Aleksandar}.

Adversarial attacks present a unique challenge for deep learning models. They exploit subtle weaknesses, causing misclassification of examples with imperceptible changes. Such attacks can have a severe impact on the reliability and safety of deep learning models, especially in safety-critical applications like autonomous driving. Adversarial examples can lead to incorrect decisions, potentially resulting in severe consequences. Therefore, it is crucial to analyze the robustness and reliability of deep learning models to ensure their safety in real-world deployments \cite {Samuel, Muhammad}. 
Explainable Artificial Intelligence (XAI) is a critical component in addressing the challenges of deep learning. It helps to enhance the understanding and trustworthiness of AI by providing interpretable and human-understandable explanations of AI decisions. The techniques, tools, and algorithms used in XAI generate explanations that help build trustworthy and interpretable deep learning models. These explanations improve trust by providing insights into the model's decision-making process, addressing challenges of trust, transparency, bias understanding, and fairness, and promoting a more robust and impartial decision-making process \cite {Mohammed, Aha, Tamer}. 

Despite recent advancements in deep learning models, research has identified gaps and shortcomings in making these models robust against adversarial attacks. It has been observed that there is an overly disproportionate focus on adversarial machine learning compared to non-adversarial robustness. Additionally, there is a significant gap in model performance when faced with naturally-induced image corruptions or alterations, which can result in performance degradation similar to that seen in adversarial conditions. This vulnerability to natural image corruptions suggests that understanding model performance on natural data should be prioritized before focusing on resilience to adversarial attack scenarios \cite {Numair, Tianhang, Wei}. 

We conducted a research to explore the challenges posed by adversarial attacks on deep learning models. Our primary focus was on understanding and quantifying the impacts of different pixels. To achieve this, we used SHAP (SHapley Additive exPlanations) within the realm of Explainable Artificial Intelligence (XAI) to unravel the complex dynamics of how deep learning models respond to adversarial manipulations. Our goal was not only to identify vulnerabilities but also to enhance the robustness and transparency of these models. The stakes are high in critical applications such as healthcare and autonomous systems. Therefore, our research strives to fortify these models against adversarial threats while simultaneously improving their interpretability and trustworthiness. Through this work, we aim to bridge the existing gap between robustness and transparency in AI, offering novel insights and methodologies that could significantly advance the field.

\section{Contributions of the Paper}

This paper introduces several significant contributions to the field of AI, particularly in enhancing the robustness and transparency of deep learning models against adversarial threats:

•	SHAP Gradient Explainer-based Pixel Analysis: Pioneering gradient explained-based pixel-level analysis for adversarial understanding.
•	Comprehensive Model Evaluation: Assessing model robustness across various adversarial perturbations and UMAP visualization is used to improve the interpretability of adversarial impact.
•	SHAP Value Comparison:  Analyze critical pixels on different thresholds to reveal subtle adversarial influences.
•	Statistical Validation: Perform statistical validation to strengthen findings in adversarial robustness research.

\begin{itemize}
    \item \textbf{SHAP Gradient Explainer-based Pixel Analysis:} Pioneering gradient explained-based pixel-level analysis for adversarial understanding.
    
    \item \textbf{Comprehensive Model Evaluation:} Assessing model robustness across various adversarial perturbations and UMAP visualization is used to improve the interpretability of adversarial impact.
    
    \item \textbf{SHAP Value Comparison:} Analyze critical pixels on different thresholds to reveal subtle adversarial influences.
    
    \item \textbf{Statistical Validation:} Perform statistical validation to strengthen findings in adversarial robustness research.

\end{itemize}

\section{Background and Related Work}

\subsection{Model Architecture and Output Formulation of Deep Neural Networks}



The following is an explanation of a deep neural network classification model denoted by \( F(\cdot) : \mathbb{R}^d \rightarrow [0, 1]^{C} \). Its purpose is to take an image with dimensions \( d = h \times w \times c \) and classify it into one of several categories (output labels) with a probability vector \( F(x) \) of dimensions \( C \). The input to the model is an image vector, and the output is a probability distribution of all possible labels, where \( F(x)_i \)  is the probability that the input vector \( x \)  is labeled with \( i \). 

The Logits of the final layer of a Deep Neural Network (DNN) is represented by \(L(\cdot)\), and the model output is calculated by using the softmax activation function on Logits. This can be expressed as \cite {ZhangS.}:

\begin{equation}
    F(x) = \text{softmax}(L(x)).
    \end{equation}

The label with the highest probability in \( F(x) \) is the predicted label of an input image \( x \), and it looks like this \cite {ZhangS.}: 
\begin{equation}
    \hat{y} = \arg\max_i (F(x)_i).
    \end{equation}

Let's assume that the deep neural network has \( l + 1 \) layers, with the input layer being the 0-th layer and the output layer being the \( l \)-th layer. The output of the \( i \)-th neural network layer is denoted as \( F_i(\cdot) \).

\subsection{Adversarial Attacks and Defense Mechanisms} 

The process used to create adversarial samples is called an adversarial attack model. It involves using a model, denoted by  \( F(\cdot) \), and an original input image, denoted by \( x \), to create an adversarial sample, denoted by \( x' = x + d \) , by introducing a specific perturbation to \( x \). This perturbation, denoted by \( d \), causes the model to predict a different label for the adversarial sample \( x' \) than it would for the original sample \( x \) \cite {ZhangS.}.

\begin{align}
    \hat{y}' &\neq \hat{y}, \\
    \text{where } \hat{y} &= \arg\max_i (F(x)_i) \text{and} \\
    \hat{y}'&= \arg\max_i (F(x')_i),
\end{align}

The perturbation \( \|d\| < \epsilon \) must have an absolute value less than epsilon, where epsilon represents the maximum perturbation size. The distance metric, denoted by  \( \|\cdot\| \) , is used to measure the perturbation, which can be quantified by three norms: \( l_0 \), \( l_2 \), and 
\( l_{\infty} \). The \( l_0 \) norm counts the number of altered non-zero elements, or the number of pixels that have been modified by the adversarial attack in the original image. The \( l_2 \) norm refers to the conventional Euclidean distance, which is obtained by summing the squares of the modified elements and then taking the square root. The \( l_{\infty} \) norm represents the highest value among all the elements of the perturbation. Recently, Wang et al. (34) used a novel distance metric that relies on Just Noticeable Distortion (JND) to more accurately assess the perceptual similarity and provide adversarial instances that are more visually realistic. By default, this article uses the 
\( l_{\infty} \) norm to quantify the magnitude of the disturbance.

Several techniques for generating adversarial examples have been developed in recent years. Some of the most prominent options that we utilize for our assessment include:


\subsubsection{Fast Gradient Sign Method (FGSM) \cite {Goodfellow.}}

Adversarial samples are generated by the Fast Gradient Sign Method (FGSM), which utilizes the linear properties of deep neural networks. In this method, the adversarial example \( x' \) is obtained by :
\begin{equation}
    x' = x + \epsilon \cdot \text{sign}(\nabla_x J(x, y));
\end{equation}

Here, \( x \) represents the original image, \( x' \) represents the adversarial example, \( J \) refers to the loss function, and \( \epsilon \) is the perturbation limit. This technique modifies the pixels uniformly based on the gradient of the loss function.



\subsubsection{Basic Iterative Method (BIM) \cite {Kurakin., Madry.}}

The Basic Iterative Method (BIM), proposed by Alexey and colleagues, is an extension of the FGSM method that progressively improves upon it. In each step, a minor disturbance is introduced by the following equation: 
\begin{equation}
    x'_{i} = \text{clip}_{x, \epsilon} \left( x'_{i-1} + \epsilon \cdot \text{sign}(\nabla_{x'_{i-1}} L(x'_{i-1}, y)) \right);
    \end{equation}
The BIM algorithm starts with an initial value of \( x'_0 = x \), and a clipping function is applied to ensure that the modified image remains within specified boundaries. The iterative nature of this method enables more precise fine-tuning compared to the FGSM method.


\subsubsection{Projected Gradient Descent (PGD) \cite {Stiff}}

PGD, an extension of FGSM, adopts a multi-step approach:
\begin{equation}
x'_0 = x \quad 
\end{equation}
\begin{equation}
x'_{t+1} = \text{clip}_{[0,1]} \left( x'_t + \alpha \cdot \text{sign}(\nabla J_\theta(x'_t, l)) \right)
\end{equation}
The \(\text{clip}_{[0,1]}(\cdot)\) function ensures the perturbed image remains within [0, 1]. PGD is computationally intensive but offers a stronger and more precise attack than FGSM.



Projected Gradient Descent (PGD) is a technique that is derived from the Fast Gradient Sign Method (FGSM).
\begin{equation}
    x'_0 = x \quad 
    \end{equation}
    \begin{equation}
    x'_{t+1} = \text{clip}_{[0,1]} \left( x'_t + \alpha \cdot \text{sign}(\nabla J(\theta, x'_t, l)) \right)
    \end{equation}

The perturbed image is then clipped to the range of [0,1] using the function \(\text{clip}_{[0,1]}(\cdot)\). This process is performed until a certain number of iterations is reached. The PGD algorithm requires a significant amount of computational resources, but it provides a more robust and accurate method of attack compared to the FGSM.


\subsubsection{DeepFool}

DeepFool \cite {Moosavi} is an algorithm that aims to generate adversarial examples with minimal perturbation. The algorithm minimizes the \(L_2\) norm and iteratively applies small perturbations to an input image using a linear neural network. 

The objective of DeepFool is to find the smallest perturbation required to change the classification of an input image. The algorithm computes the perturbation by solving the following optimization problem:

\begin{equation}
r^*(x_0) = \arg\min_r \|r\| \quad \text{s.t.} \quad \text{sign}(f(x_0 + r)) \neq \text{sign}(f(x_0)),
\end{equation}

The solution to this problem can be approximated using the gradient of the neural network. When the gradient is not available, DeepFool uses an approximation based on the linearization of the neural network. The perturbation is computed as follows:

\begin{equation}
r^*(x_0) = \frac{|f(x_0)|}{\|w\|^2} w.
\end{equation}

DeepFool can also be used to generate targeted adversarial examples. In this case, the algorithm solves the following optimization problem:

\begin{equation}
\arg\min_{r_i} \|r_i\|^2 \quad \text{s.t.} \quad f(x_i) + \nabla f(x_i)^T r_i = 0.
\end{equation}

DeepFool has been shown to be effective in generating adversarial examples with comparable accuracy to FGSM but with less perturbation.

According to the research \cite {Ilyas.}, the existence of adversarial examples in a dataset is an intrinsic property. The authors introduce the concept of robust and non-robust characteristics to classify dataset features. Non-robust features exhibit high predictive power but also exhibit high susceptibility to significant changes when slight perturbations are introduced to the input. Conversely, robust features possess high predictive power and are resistant to slight changes in the input. Robust features are considered as fundamental aspects of the target class, such as the presence of wheels and windows in cars. Non-robust features are seemingly arbitrary patterns that are not easily discernible by humans but demonstrate strong predictive power during the training phase. The work of \cite {Ilyas.} illustrates that non-robust features play a significant role in the generation of adversarial examples and their ability to move between different classification models. These non-robust features are vulnerable to minor perturbations in the input, which can result in significant changes in the value of these highly predictive features.

Some examples of defense methods against adversarial example include adversarial training \cite {Goodfellow.}, Defensive Distillation \cite {Papernot}, Gradient Obfuscation, Feature Squeezing \cite {Evans}, ML-LOO \cite {Yang}, Density Estimates \cite {Feinman}, Local Intrinsic Dimensionality \cite {Ma}, Mahalanobis Distance 
 \cite {Lee}, and many more. In our research, we propose a method for identifying critical pixels in both normal and adversarial examples. This will aid in the generation and defense of attacks. 


\subsection{Explainable AI}

Explainable AI (XAI) plays a crucial role in the field of machine learning as it enables AI systems to be transparent and comprehensible. Its main objective is to establish confidence and effectively handle complex AI solutions. The role of XAI is particularly important in supervised models as it enables a comprehensive explanation of the reasoning behind predictions. Prominent XAI technique is SHAP, used for additive feature attribution. It involves explaining Shapley values, which are a way to assign credit to input features for the output of a model. SHAP combines the Shapley value method with the local interpretable model-agnostic explanations (LIME) \cite {Ribeiro} technique. Many explanation methods for deep learning models use additive feature attribution methods, such as LIME, DeepLIFT \cite {Shrikumar}, layer-wise relevance propagation, Shapley regression values, Shapley sampling values and quantitative Input Influence. There are several explainers in SHAP \cite {LundbergandS}, including the tree explainer, gradient explainer, kernel explainer, deep explainer and sampling explainer. The tree explainer is a fast and exact method for estimating SHAP values for tree models and ensembles of trees. The gradient explainer can explain a model using expected gradients. The kernel explainer uses a particular weighted linear regression to compute the importance of each feature. The deep explainer is an enhanced version of the DeepLIFT algorithm that approximates the conditional expectations of SHAP values using a selection of background samples. The sampling explainer is an extension of the Shapley sampling values explanation method \cite{ZhangK}. Our work centers around a thorough pixel-level examination using the Gradient Explainer method, providing a unique perspective on adversarial understanding.


% It is widely used in various domains such as engineering \cite{Mangalathu}, aerospace \cite{Aouf}, medical \cite{Singh},  transportation \cite{Parsa}, manufacturing \cite{Huang}, security \cite{Warnecke}, and others due to its ability to analyze complex models. 

% DeepSHAP has proven to be crucial in detecting adversarial examples. It is used in image detection to differentiate between adversarial and normal samples, as well as for identifying word-level adversarial attacks.

Our study expands the applicability of SHAP values beyond being just a general XAI indicator. Instead, we use SHAP values to specifically pinpoint crucial pixels, thus increasing our understanding of adversarial attack manipulations and strengthening the resilience of AI models.




\section{Proposed Methodology:}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{paper_images/model.pdf}
    \caption{Overview of the Proposed Model}
    \label{overview}
\end{figure*}
   
Our research, illustrated in Figure. \ref{overview}, aims to improve the defense strategies of deep learning systems against complex attacks. We accomplish this by training Convolutional Neural Networks with the MNIST dataset and focusing on correctly identifying images. To test the strength of our model, we carry out a series of sophisticated simulated attacks to challenge the system's decision-making abilities in different scenarios. We then conduct a comprehensive analysis to determine the effect of each step size on the model's decision-making process, particularly during these simulated attacks. In the final phase, we use a specialized technique to identify the most critical pixels in attack generation and detection scenarios. Additionally, we employ a statistical method called the t-test to evaluate and confirm the significance of these identified critical points. This process ensures the accuracy and dependability of our findings. Our model's multi-stage, comprehensive approach protects against a wide range of adversarial threats in deep learning environments.


\subsection{Stage 1: Data and Model Preparation}

In the initial stage of our investigation, we used CNN, which had already been trained on the MNIST dataset, to classify digits. A crucial step prior to the study was to carefully select cases that the model could accurately classify, resulting in a dataset comprising only correctly predicted instances. This aimed to help us understand how adversarial perturbations impact a model performing well otherwise.
We chose a dataset with a varied class distribution that accurately reflected real-world scenarios. The dataset consisted of 973 instances labeled as 0, 1133 as 1, 1016 as 2, 989 as 3, 969 as 4, 882 as 5, 937 as 6, 1005 as 7, 946 as 8, and 984 as 9. Although the digit frequencies were not perfectly uniform, we acknowledged this slight imbalance to ensure the integrity of our study. Rather than using a synthetically balanced environment, we wanted to test the model's ability to counter adversarial attacks in a real-world scenario. This approach allowed us to focus on the model's robustness in situations it would encounter in the real world.

To ensure that our study focused on how adversarial attacks impact a model already performing well, we focused on accurately classified cases. Our systematic methodology in data selection formed a well-defined foundation for our investigation, enabling us to examine the ability of neural networks to tolerate adversarial manipulations in a way that closely resembles real-world scenarios.


\subsection{Stage 2: Adversarial Attack Simulation}

In the previous stage, we created adversarial examples to test how reliable the model is. Our objective was to evaluate the performance of the neural network model when it is exposed to misleading conditions deliberately. We chose a range of epsilon values, which were set to [0.03, 0.04, 0.05, 0.1, 0.2, 0.25], to create different levels of disturbance intensities.

Through this approach, we could comprehensively examine how the model reacted to different degrees of adversarial intensity. This is essential to determine its overall robustness. We used Foolbox, a Python library, to aid in creating a broad range of adversarial attacks.

We executed various types of adversarial attacks, including LinfBasicIterativeAttack (BIM), LinfFastGradientAttack (FGSM), LinfDeepFoolAttack, and LinfProjectedGradientDescentAttack (PGD). We systematically gathered and analyzed the adversarial examples created from these attacks.
Figure. \ref{fig:umap} shows a UMAP visualization of the MNIST dataset, where each point represents a digit from 0 to 9. The colors represent the original digit classes, while the adversarial examples produced by PGD attacks are interspersed within these clusters. This visualization helps us understand how adversarial perturbations affect classification accuracy and identify vulnerabilities in the model. By gaining insights from this analysis, we can improve the model's defense and ensure its reliability against adversarial threats in practical scenarios.


\begin{figure*}[!ht]
    \centering
    \begin{subfigure}{.50\textwidth}
        \centering
        \includegraphics[width=\linewidth]{paper_images/1.png}
        \caption{Original MNIST digit classes}
        \label{fig:umap_original}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{.50\textwidth}
        \centering
        \includegraphics[width=\linewidth]{paper_images/2.png}
        \caption{Original and PGD attack}
        \label{fig:umap_pgd}
    \end{subfigure}
    \caption{UMAP projections illustrating the separation of digit classes within the MNIST dataset and the integration of adversarial examples.
     The left panel (\ref{fig:umap_original}) shows the natural clustering of MNIST digits, and the right panel panel (\ref{fig:umap_pgd}) overlays PGD 
     adversarial examples.}
    \label{fig:umap}
\end{figure*}



\subsection{Stage 3: Extraction of XAI Signatures}
In this stage, we aimed to analyze the influence of input pixels on model predictions. We examined both standard and adversarial altered data. We used Deep Explainer to highlight the contribution of each pixel towards the model's output. By calculating SHAP values for specific subsets of our data, we could compare the effect of standard inputs versus adversarial inputs. This approach helped us identify significant differences in their influence on the model's predictions. It allowed us to gain valuable insights into the model's decision-making process when presented with adversarial conditions.

\begin{figure}[h]
\centering
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\linewidth]{paper_images/normal.png}
\label{fig:correct_shap}
\end{subfigure}
\par\medskip
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\linewidth]{paper_images/pgd.png}
\label{fig:pgd}
\end{subfigure}
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\linewidth]{paper_images/fgsm.png}
\label{fig:fgsm}
\end{subfigure}
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\linewidth]{paper_images/deep.png}
\label{fig:deep}
\end{subfigure}
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\linewidth]{paper_images/basic.png}
\label{fig:basic}
\end{subfigure}
\caption{SHAP heatmaps depicts the interpretability of a model's decision for a normally classified digit and its progressive alteration under various adversarial attacks (PGD, FGSM, deepfool and basic iterative method), highlighting the shift in influential pixels at a granular level. (a): Normal, (b): PGD, (c): FGSM, (d): Deep Fool, (e): BIM}
\label{fig:both_shap_figures}
\end{figure}



    
\section{Stage 4: Identification and Validation of Critical Pixels}

\subsection{SHAP Values for Critical Pixel Analysis}
    In Stage 4, we use the SHAP values that we extracted earlier to pinpoint and confirm the pixels that have a significant impact on the 
    model's predictions. This step is essential for gaining a deeper understanding of the model's behavior and developing effective strategies 
    to make it more resilient to adversarial attacks.
    The process involves:

 \subsubsection{Attack Generation Pixel Analysis}
We first analyze the SHAP values for adversarial image pixels. This step is crucial to identify which pixels, when altered, are most effective 
in generating successful adversarial attacks. By understanding the pixels that significantly influence the model's erroneous decisions, we can 
uncover how adversarial perturbations are guided and how they can be strategically crafted.

\subsubsection{Attack Detection Pixel Analysis}
Subsequently, we focus on detecting these adversarial manipulations by examining the differences in SHAP values between normal and adversarial
images. This comparison highlights the pixels where the largest deviations occur, signaling potential areas of the image being exploited by 
adversarial attacks. This analysis is vital for developing robust detection mechanisms that can identify and counteract these manipulative 
changes.

\subsubsection{Original Image Pixel Importance}
Alongside these analyses, we also scrutinize the SHAP values of original, unmanipulated images. This examination is essential to establish a 
baseline of the model’s interpretation of unperturbed images. Understanding the importance of pixels in the original context provides a 
reference point, helping us to better interpret the changes observed in the adversarial context.

Through this comprehensive methodology, we aim to provide a deep understanding of how pixel-level manipulations affect AI decision-making, 
both in generating and detecting adversarial attacks, as well as understanding the inherent behavior of the model under normal conditions.
 
        
        
\begin{table*}[t]
    \small
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Threshold} & \textbf{FGSM} & \textbf{BIM} & \textbf{DeepFool} & \textbf{PGD} \\
    \hline
    No Modification & 0 & 0 & 0 & 0 \\
    \hline
    Threshold 1 & 2 & 3 & 1 & 4 \\
    \hline
    Threshold 2 & 5 & 7 & 2 & 8 \\
    \hline
    Threshold 3 & 9 & 12 & 4 & 13 \\
    \hline
    Threshold 4 & 15 & 18 & 7 & 20 \\
    \hline
    \end{tabular}
    \caption{Understanding Model Vulnerability: Influence of Critical Pixel Modification on Attack Identification and Model Robustness}
    \label{tab:model_robustness_attacks}
    \end{table*}
    
\subsubsection{Statistical Validation of Critical Pixels}
To substantiate the distinction between critical and non-critical pixels identified in our analysis, we conducted a statistical t-test on the
 SHAP values. This test helps validate the significance of the differences observed in the SHAP values between critical and non-critical pixels.
  The implementation of a t-test ensures that the observed disparities are not due to random chance, thereby lending statistical rigor to our 
  findinßgs and confirming the reliability of our pixel importance assessments.


    
    \begin{figure}[h]
        \centering
        \begin{subfigure}{\columnwidth}
            \centering
            \includegraphics[width=\linewidth]{paper_images/fgsm_c.png}
            \caption{FGSM}
            \label{fig:fgsm_c}
        \end{subfigure}
        \par\medskip % Adds space between the two subfigures
        \begin{subfigure}{\columnwidth}
            \centering
            \includegraphics[width=\linewidth]{paper_images/deep_c.png}
            \caption{Deepfool}
            \label{fig:deep_c}
        \end{subfigure}
        \begin{subfigure}{\columnwidth}
            \centering
            \includegraphics[width=\linewidth]{paper_images/basic_c.png}
            \caption{Basic Iterative Method}
            \label{fig:fgsm}
        \end{subfigure}
        \begin{subfigure}{\columnwidth}
            \centering
            \includegraphics[width=\linewidth]{paper_images/pgd_c.png}
            \caption{PGD}
            \label{fig:deep}
        \end{subfigure}
     
        \caption{Visualization of critical pixels in adversarial samples.}
        \label{fig:both_shap_figures}
    \end{figure}
    
    % \begin{table*}[t]
    %     \centering
    %     \begin{tabular}{|p{3cm}|p{6cm}|}
    %     \hline
    %     \textbf{SHAP Value Proximity to Zero} & \textbf{Interpretation} \\ \hline
    %     Close to Zero & 
    %     \begin{itemize}
    %     \item Feature has minimal impact on the model's prediction.
    %     \item Changes in the feature value do not significantly affect the prediction.
    %     \item Feature is considered neutral or weak in terms of prediction influence.
    %     \end{itemize}
    %     \\ \hline
    %     Far from Zero (Positive or Negative) & 
    %     \begin{itemize}
    %     \item Feature has a substantial impact on the model's prediction.
    %     \item Variations in the feature value lead to significant changes in the prediction.
    %     \item Feature is a strong influencer of the predicted outcome.
    %     \item The magnitude of the SHAP value indicates the strength of the feature's influence.
    %     \end{itemize}
    %     \\ \hline
    %     \end{tabular}
    %     \caption{Interpretations of SHAP Values Based on Proximity to Zero}
    %     \label{table:shap-value-interpretation}
    % \end{table*}
    
    

% In Stage 5 of our research, we focused on the development of a binary classification model that could effectively differentiate between original and adversarial examples. The crux of this stage was the utilization of XAI signatures, particularly SHAP values, instead of raw data. The choice of XAI signatures was strategic and stemmed from the need for deeper insights into the decision-making processes of neural networks. These signatures, unlike raw data, provide a nuanced understanding of feature importance and contributions, making them immensely valuable in identifying subtle manipulations in adversarial examples.

% This approach aligns with the broader objective of enhancing the interpretability and trustworthiness of deep learning models. By using XAI signatures, we aim to gain a more transparent view into how and why models make certain decisions, especially under adversarial conditions. This transparency is crucial for identifying potential vulnerabilities and strengthening the model's defenses against sophisticated attacks.

% Moreover, understanding the influence of individual features on model predictions through XAI signatures aids significantly in the research and development of more robust attack detection methods. It enables us to pinpoint which features are most exploited in adversarial attacks and thus devise more effective strategies to counter them.

% For the testing phase, we opted for a simple neural network model. This choice was deliberate; a simpler model allows for clearer interpretation of results and a more straightforward evaluation of the effectiveness of XAI signatures. It serves as a preliminary testing ground, where we can assess the viability of our approach before scaling up to more complex models. This step is essential in ensuring that our methodology is sound and effective in a controlled environment before applying it to more intricate and real-world scenarios.
% Evaluate the classification model's performance in correctly identifying adversarial attacks.

\section{Experiments:}

In our evaluation we aimed to answer the following two research questions:
\begin{itemize}
    \item\textbf{RQ1:} How effective is SHapley Additive exPlanations (SHAP) analysis in identifying specific pixel-level vulnerabilities in deep learning models, and what insights does it provide for understanding the nature of these vulnerabilities?
    \item\textbf{RQ2:} In what ways can the findings from SHAP-based fine-grained pixel analysis be applied to enhance the robustness of deep learning models against sophisticated adversarial attacks?
\end{itemize}
\subsection{Case Study 1: Evaluating Model Robustness Against Different Adversarial Attacks}

During our experimentation phase, we conducted a thorough examination to test the strength of our model against a range of adversarial attacks, each with varying levels of perturbation. We aimed to understand the extent to which these attacks could potentially deceive or mislead our model. We used the MNIST dataset to train a Convolutional Neural Network (CNN), which achieved 98\% accuracy on testing examples. However, to test its robustness further, we chose examples that CNN had previously classified correctly and introduced four types of attacks: PGD, BIM, FastGradient, and Deepfool.

The table \ref{tab:robust_accuracy} provides a detailed overview of our model's performance when subjected to different adversarial attacks. Each attack is characterised by varying levels of perturbation, which are quantified by epsilon values of 0.03, 0.04, 0.05, 0.1, 0.2, and 0.25. As the level of perturbation increases, the accuracy of the model decreases, highlighting its vulnerability to adversarial perturbations. The PGD and BIM attacks proved remarkably effective, reducing the model's accuracy to zero at high perturbation levels (epsilon values of 0.2 and 0.25). This observation emphasises the model's susceptibility to specific adversarial strategies.

The performance of the model has been evaluated in detail in Table \ref{tab: metrics}. The table includes precision, recall, and F1-Score for each attack type at different epsilon values. These metrics offer a more detailed look at how well the model balances between minimizing false positives and false negatives under different adversarial conditions.

Although the Fast Gradient Sign Method (FGSM) may appear to outperform other attack methods in specific scenarios, it is crucial to recognize that the metrics shown in the table are relatively moderate. This implies that, despite the model achieving some level of accuracy or recall, it still needs to attain optimal performance in adversarial circumstances.



\begin{table}[ht]
    \centering
    \caption{Robust Accuracy for Different Perturbations}
    \label{tab:robust_accuracy}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{L\ensuremath{_{\infty}}} & \textbf{PGD} & \textbf{BIM} & \textbf{FastGradient} & \textbf{Deepfool} \\ \hline
     0.03        & 95.4\%       & 95.4\%       & 97.1\%               & 97.1\%           \\ \hline
     0.04        & 91.2\%       & 91.0\%       & 95.5\%               & 95.4\%           \\ \hline
     0.05        & 84.9\%       & 84.7\%       & 92.9\%               & 92.7\%           \\ \hline
     0.1         & 22.5\%       & 23.8\%       & 69.5\%               & 69.0\%           \\ \hline
     0.2         & 0.0\%        & 0.0\%        & 17.9\%               & 17.6\%           \\ \hline
     0.25        & 0.0\%        & 0.0\%        & 6.6\%                & 7.2\%            \\ \hline
    \end{tabular}
    \end{table}

   
    
    \begin{table}[htbp]
    \centering
    \caption{Metrics for Different Epsilon and Attack Types}
    \begin{tabular}{cccccc}
    \toprule
    Epsilon & Attack Type & Precision & Recall & F1-Score \\
    \midrule
    0.03 & BIM & 0.954646 & 0.953601 & 0.953749 \\
    \cmidrule(lr){2-5}
    & Deepfool & 0.970903 & 0.9702 & 0.970362 \\
    \cmidrule(lr){2-5}
    & FGSM & 0.971445 & 0.970721 & 0.970891 \\
    \cmidrule(lr){2-5}
    & PGD & 0.954837 & 0.95386 & 0.954011 \\
    \midrule
    0.04 & BIM & 0.911803 & 0.909292 & 0.909065 \\
    \cmidrule(lr){2-5}
    & Deepfool & 0.954805 & 0.953713 & 0.953828 \\
    \cmidrule(lr){2-5}
    & FGSM & 0.955764 & 0.954631 & 0.95476 \\
    \cmidrule(lr){2-5}
    & PGD & 0.913378 & 0.911083 & 0.911041 \\
    \midrule
    0.05 & BIM & 0.849694 & 0.844652 & 0.843379 \\
    \cmidrule(lr){2-5}
    & Deepfool & 0.92858 & 0.926484 & 0.926312 \\
    \cmidrule(lr){2-5}
    & FGSM & 0.929736 & 0.927618 & 0.927463 \\
    \cmidrule(lr){2-5}
    & PGD & 0.851144 & 0.847073 & 0.846147 \\
    \midrule
    0.1 & BIM & 0.264287 & 0.237587 & 0.241945 \\
    \cmidrule(lr){2-5}
    & Deepfool & 0.686949 & 0.686049 & 0.67418 \\
    \cmidrule(lr){2-5}
    & FGSM & 0.694292 & 0.690916 & 0.679787 \\
    \cmidrule(lr){2-5}
    & PGD & 0.256003 & 0.226307 & 0.231078 \\
    \midrule
    0.2 & BIM & 0 & 0 & 0 \\
    \cmidrule(lr){2-5}
    & Deepfool & 0.180356 & 0.17113 & 0.163833 \\
    \cmidrule(lr){2-5}
    & FGSM & 0.185497 & 0.174777 & 0.165671 \\
    \cmidrule(lr){2-5}
    & PGD & 0 & 0 & 0 \\
    \midrule
    0.25 & BIM & 0 & 0 & 0 \\
    \cmidrule(lr){2-5}
    & Deepfool & 0.0644666 & 0.0686077 & 0.0622584 \\
    \cmidrule(lr){2-5}
    & FGSM & 0.0580293 & 0.0636508 & 0.0558428 \\
    \cmidrule(lr){2-5}
    & PGD & 0 & 0 & 0 \\
    \bottomrule
    \end{tabular}
    \label{tab:metrics}
    \end{table}
    
    
  
        
% \begin{table}[ht]
%     \centering
%     \caption{Robust Accuracy for Perturbations}
%     \label{tab:robust_accuracy}
%     \begin{tabular}{|c|c|c|c|c|}
%     \hline
%     Epsilon & PGD & BIM & FGSM & Deepfool \\
%     \hline
%     0.03 & 95.4\% & 95.4\% & 97.1\% & 97.1\% \\
%     0.04 & 91.2\% & 91.0\% & 95.5\% & 95.4\% \\
%     0.05 & 84.9\% & 84.7\% & 92.9\% & 92.7\% \\
%     0.1 & 22.5\% & 23.8\% & 69.5\% & 69.0\% \\
%     0.2 & 0.0\% & 0.0\% & 17.9\% & 17.6\% \\
%     0.25 & 0.0\% & 0.0\% & 6.6\% & 7.2\% \\
%     \hline
%     \end{tabular}
%     \end{table}
    

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{paper_images/Result1.png}
%     \caption{Assessing Model Robustness Across Various Epsilon Values Under Four Types of Attacks}
%     \label{fig:robust_accuracy_attacks}
% \end{figure}

% \begin{table}[ht]
%     \centering
%     \caption{Summary of Total Misclassifications for Various Adversarial Attacks}
%     \label{tab:misclassifications}
%     \begin{tabular}{|l|c|c|c|c|c|c|}
%     \hline
%     & \multicolumn{6}{c|}{\textbf{Epsilon}} \\ \cline{2-7} 
%     \multirow{-2}{*}{\textbf{Attack Type}} & \textbf{0.03} & \textbf{0.04} & \textbf{0.05} & \textbf{0.10} & \textbf{0.20} & \textbf{0.25} \\ \hline
%     BIM & 451 & 881 & 1508 & 7490 & 9834 & 9834 \\ \hline
%     FGSM & 284 & 440 & 702 & 2998 & 8070 & 9186 \\ \hline
%     Deepfool & 289 & 449 & 713 & 3045 & 8104 & 9130 \\ \hline
%     PGD & 451 & 881 & 1483 & 7616 & 9834 & 9834 \\ \hline
%     \end{tabular}
% \end{table}

The findings emphasize the importance of implementing advanced defensive measures to enhance the robustness of machine learning models against 
adversarial attacks. The observations made in this case study can help in the creation of such defenses, especially in addressing the weaknesses exposed by BIM and PGD when dealing with smaller perturbations, and the slower but eventual impact of FGSM and DeepFool when dealing with higher perturbations.


\subsection{Pixel-Level Analysis using Gradient Explainer}
 
  This case study delves into a novel approach to bolster model robustness, integrating SHAP (SHapley Additive exPlanations) analysis with pixel categorization and manipulation. Unlike traditional applications of SHAP, our methodology ventures into pixel categorization, critical pixel thresholds, and their impact on model resilience. We investigate the interplay between critical pixels and model accuracy under adversarial challenges, focusing on scenarios where the model's performance is compromised. By employing 200 samples for each adversarial attack and correct image across six epsilon values, we evaluate over tens of thousands of data points. Our findings shed light on the intricate dynamics between critical pixels and model robustness, presenting new avenues for enhancing deep learning model defenses against adversarial attacks.

  In response to this growing concern, our case study explores an innovative approach to bolster model robustness. While SHAP analysis and SHAP signatures have been pivotal in model explanation and enhancement, our study extends these concepts to encompass pixel categorization and manipulation. We introduce the concept of "critical pixels" those that exert a profound influence on model predictions. By setting critical pixel thresholds, we aim to achieve precision levels exceeding 70\%, 80\%, or 90\% within the SHAP signature.

  Our experimental design encompasses both normal and adversarial images, comprising 200 samples for each attack and correct image across six epsilon values. This meticulous dataset design allows for a comprehensive evaluation of model behavior under varying adversarial conditions. Notably, we exclusively select adversarial examples that confound the model, ensuring our focus on scenarios where model accuracy is compromised.
  
  The results of our study reveal the intricate relationship between critical pixels and model robustness. By modifying critical pixels in adversarial images, we investigate whether such alterations can resuscitate model accuracy. Simultaneously, we analyze the impact of critical pixel modifications in regular images, exploring whether such changes detrimentally affect model accuracy.
  
  Our findings suggest that critical pixels indeed play a pivotal role in influencing model behavior under adversarial conditions. The interplay between these critical pixels and model robustness is nuanced, with critical pixel manipulation serving as a potential avenue for model defense. However, our study also underscores the delicate balance between critical pixel alterations and model performance in regular images, revealing the need for a nuanced approach to model enhancement.
  
%   \begin{figure}
%     \centering
%     \includegraphics[width=0.9\linewidth]{paper_images/correctinf.png}
%     \caption{Degradation of Model Accuracy with Increased Critical Pixel Thresholds on Normal Images}
%     \label{Degradation}
% \end{figure}
%   \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{paper_images/critical_adv.png}
%     \caption{Recovery of Model Accuracy through Critical Pixel Manipulation in Adversarial Images (Epsilon = 0.25)}
%     \label{Recovery}
% \end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{deepexplainer/correctinf.png}
    \caption{Pixel Importance Deep}
\end{figure}
\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{deepexplainer/deepadv.png}
    \caption{Pixel Importance Deep}
\end{figure*}



Figure.\ref{Degradation} represents the impact on model accuracy when modifying normal image pixels based on different levels of influence and thresholds. The expectation is that the accuracy would decrease from 100\% as critical pixels are altered.
Figure.\ref{Recovery} represents the impact on model accuracy when modifying adversarial image pixels to restore accuracy. The plot specifically deals with adversarial examples generated with an epsilon value of 0.25, and the goal is to observe how the accuracy increases from 0\% upwards.
 
In Figure.\ref{Degradation}, the observed trend is an expected decrease in model accuracy as the threshold for critical pixel influence increases. This decrease is pronounced within the 'absolute' influence type, suggesting that indiscriminate alteration of pixels deemed critical—without considering the nature of their influence—can significantly degrade model performance. Notably, the transition from the 80\% to the 90\% threshold underscores the sensitivity of the model to pixel-level changes, particularly for pixels with a 'positive' influence, which seems to play a substantial role in maintaining the model's high accuracy on standard images.

Figure.\ref{Recovery} offers insights into the recovery of accuracy for adversarial images by modifying critical pixels, with an emphasis on samples generated with an epsilon value of 0.25. The model's response to pixel manipulation varies across different attack types, which implies that the efficacy of such defensive measures is contingent on the attack's characteristics. It's particularly interesting that for the 'absolute' influence type, a broader definition of critical pixels (as seen at the 70\% threshold) aids in mitigating the effects of simpler attacks like BIM and FGSM. However, for more complex attacks such as Deepfool and PGD, even high precision in identifying critical pixels does not significantly bolster the model's resilience.

These plots collectively underscore the complexity inherent in enhancing model robustness through pixel manipulation. While adjusting critical pixels shows promise as a line of defense, its success is clearly attack-dependent. The findings advocate for a strategic approach to applying SHAP-based pixel modifications, where the type of pixel influence—be it absolute, positive, or negative—must be carefully considered to avoid diminishing model accuracy or failing to recover it in the face of adversarial challenges.

The case study, therefore, highlights the importance of a multifaceted defense strategy that incorporates pixel-level interventions alongside other techniques to construct a robust defense against a spectrum of adversarial strategies. Given that the analysis is based on a fixed epsilon value, further research should encompass a wider array of attack intensities to ensure broader applicability and resilience of the proposed defense mechanisms.
  

  \subsection{Case Study 3: Validation of Identified Critical Pixels}
    In Section VI, we discussed the critical pixels identified through our methodology for both attack generation and detection. To determine their significance, we conducted a rigorous statistical validation using a two-sample t-test. This validation step is crucial to highlight the critical role played by these pixels in shaping the decision-making process of our machine learning model.

    Our approach involved a comprehensive examination of SHAP values, with specific emphasis on those related to attack detection. SHAP values are useful in understanding the inner workings of our model, and they shed light on the pixels that significantly influence its predictions. To streamline the analysis, we categorized these SHAP values into two distinct groups: critical pixels and non-critical pixels. We based this categorization on a set of indices derived from our methodology.
    Figure \ref{fig:bar_chart} displays a bar chart that compares the means of two groups, highlighting the difference in mean SHAP values for 'Critical' and 'Non-Critical' pixels. This visual representation is critical to our analysis.   
    Figure \ref{fig:histograms} shows histograms of SHAP values that provide a closer look at the distribution of SHAP values for both 'Critical' and 'Non-Critical' pixels. These histograms help us to understand the data distribution and its impact on statistical tests.    
    The validation of our study primarily depends on the comparison of two groups using statistical analysis. We used the two-sample t-test, a well-known method for identifying significant differences between two sets of data. Our results were significant with a computed t-statistic of 11.5968 and an extremely low p-value of $1.34 \times 10^{-30}$. Figures \ref{fig:bar_chart} and \ref{fig:histograms} show the statistical indicators that confirm the crucial role of critical pixels in our model's behavior. These pixels influence both the generation and detection of adversarial attacks.
    
 
        \begin{figure}
            \centering
            \includegraphics[width=0.9\linewidth]{deepexplainer/ttest.png}
            \caption{Pixel Importance Deep}
        \end{figure}
     
        \begin{figure}
            \centering
            \includegraphics[width=0.9\linewidth]{deepexplainer/ttest2.png}
            \caption{Pixel Importance Deep}
        \end{figure}

        
% \section{Future Directions}

% By leveraging our pixel-level analysis methodology, we have identified several critical areas for future research that can extend and enhance our understanding of AI's robustness and interpretability:

% \noindent \textbf{Complex and High-Dimensional Datasets:}
% \begin{itemize}
%     \item To test the scalability and adaptability of our approach in more complex scenarios than MNIST, future research should concentrate on utilizing our pixel-level analysis on high-dimensional datasets.
%     \item This could be achieved by exploring datasets such as high-resolution medical images or real-world surveillance data.
% \end{itemize}

% \noindent \textbf{Advances in Adversarial Attack and Defense Strategies:}
% \begin{itemize}
%     \item It is necessary to develop more advanced adversarial attack techniques and defense mechanisms that are informed by pixel-level analysis.
%     \item Such developments could provide a better understanding of vulnerabilities in AI and lead to more robust AI models.
% \end{itemize}

% \noindent \textbf{Application to Various AI Architectures:}
% \begin{itemize}
%     \item Exploring the utility of SHAP analysis in various AI architectures, including RNNs and GNNs, could reveal new insights into model behavior across different domains and data types.
% \end{itemize}

% \noindent \textbf{Automated SHAP Value Analysis for Large-Scale Applications:}
% \begin{itemize}
%     \item Automated SHAP value analysis tools are critical for scaling AI applications.
% \end{itemize}

% \noindent \textbf{Cross-Domain Adaptability and Ethical Implications:}
% \begin{itemize}
%     \item Investigating the cross-domain adaptability of our approach and its ethical implications, particularly in sensitive areas like healthcare and law enforcement, would be invaluable.
%     \item This includes understanding how different stakeholders interpret and interact with SHAP-based insights.
% \end{itemize}

% \noindent Our pixel-level analysis methodology can be a foundational step for creating adaptable and trustworthy AI systems across various domains and applications.

\section{Conclusion:}
This research conducted a thorough investigation into adversarial attack simulations, analyzing the vulnerabilities of deep learning models at the pixel level. By analyzing these attacks in detail, the researchers gained insights into their intricacies. Afterwards, the power of SHAP (SHapley Additive exPlanations) analysis was used to extract Shapley values, providing a deep understanding of how each pixel contributes to model predictions. With these Shapley signatures, critical pixels within the model's decision-making process were identified. This sequential approach, starting with attack simulations, followed by Shapley analysis, and culminating in identifying critical pixels, has enriched our comprehension of adversarial threats and laid the foundation for enhancing the robustness of AI models. In conclusion, this study significantly advances our knowledge in this domain. It offers a comprehensive framework for strengthening AI models against adversarial intrusions, thereby contributing to the broader field of deep learning security.


\begin{thebibliography}{01}

    \bibitem{Saad}S. Saad, Y. Yilin, T. Haiman, T. Yudong, P. R. Maria, S. Mei-Ling, C. Shu-Ching, and S. Iyengar. "A survey on deep learning: A. Sundaraja, “ACM Computing Surveys CSUR 51 no,” 2018.
    \bibitem{Md} Z. Md, M. T. Tarek, Y. Chris, W. Stefan, S. Paheding, S. N. Mst, H. Mahmudul, C. V. E. Brian, 
    \bibitem{Numair}S. Numair, S. Ilya, and U. Mathias, “A systematic review of robustness in deep learning for computer vision Mind the gap,” 2021.
    \bibitem{Aleksandar}M. Aleksandar, S. Ludwig, T. Dimitris, and V. Adrian, “Towards deep learning models resistant to adversarial attacks,” 2017.
    \bibitem{Samuel}H. Samuel and N. Peyman, “Opportunities and challenges in deep learning adversarial robustness A survey,” 2020.
    \bibitem{Muhammad}U. Muhammad, Q. Junaid, U. J. Muhammad, A.-F. Ala, T. H. Dinh, and Niyato. "Challenges and countermeasures for adversarial attacks on deep reinforcement learning Dusit, “IEEE Transactions on Artificial Intelligence 3 no,” 2021.
    \bibitem{Mohammed}B. Mohammed, “Peeking inside the blackbox a survey on explainable artificial intelligence XAI,” 2018.
    \bibitem{Aha}Aha. "DARPA’s explainable artificial intelligence (XAI) program David, “AI magazine 40 no,” 2019.
    \bibitem{Tamer}A. Tamer, E.-S. Shaker, M. Khan, M. A.-M. Jose, C. Roberto, G. Riccardo, D. S. Javier, D.-R. Natalia, and H. Francisco, “Explainable Artificial Intelligence XAI What we know and what is left to attain Trustworthy Artificial Intelligence,” 2023.
    \bibitem{Tianhang}Z. Tianhang, Q. Zhan, and Liu. "Adversarial attacks and defenses in deep learning Xue, “Engineering 6 no,” 2020.
    \bibitem{Wei}E. Wei, Z. S. Quan, A. Ahoud, and Li. "Adversarial attacks on deep-learning models in natural language processing: A. survey Chenliang, “ACM Transactions on Intelligent Systems and Technology TIST 11 no,” 2020.

    \bibitem{ZhangS.}
    Zhang, S., Chen, S., Liu, X., Hua, C., Wang, W., Chen, K., Zhang, J. and Wang, J., 2021. Detecting adversarial samples for deep learning models: a comparative study. IEEE Transactions on Network Science and Engineering, 9(1), pp.231-244.

    \bibitem{Goodfellow.}
    I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
    adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.

    \bibitem{Kurakin.}A.Kurakin,I.Goodfellow,andS.Bengio,“Adversarialmachinelearning at scale,” 2016.
    \bibitem{Madry.}A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep learning models resistant to adversarial attacks,” arXiv preprint arXiv:1706.06083, 2017. 

    \bibitem{Stiff}Stiff, Harald. "Explainable AI as a Defence Mechanism for Adversarial Examples." (2019)

    \bibitem{Moosavi}S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: A simple and accurate method to fool deep neural networks,” in Proc.
IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 2574–2582.

\bibitem{Ilyas.}A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry,
“Adversarial examples are not bugs, they are features,” arXiv preprint
arXiv:1905.02175, 2019.

\bibitem{Papernot}
N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation as a defense to adversarial perturbations against deep neural networks,” in 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 2016, pp. 582–597.
\bibitem{Evans}
W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
examples in deep neural networks,” arXiv preprint arXiv:1704.01155,
2017.

\bibitem{Yang} P. Yang, J. Chen, C.-J. Hsieh, J.-L. Wang, and M. I. Jordan, “ML-LOO:
Detecting adversarial examples with feature attribution,” in Proc. 34th AAAI Conf. Artif. Intell., 32nd Innov. Appl. Artif. Intell. Conf., and 10th AAAI Symp. Edu. Adv. Artif. Intell., 2020, pp. 6639–6647.
\bibitem{Feinman} R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner, “Detecting adversarial samples from artifacts,” CoRR, 2017, arXiv: 1703.00410.
\bibitem{Ma} X. Ma et al., “Characterizing adversarial subspaces using local intrinsic dimensionality,” in Proc. Int. Conf. Learn. Representation, 2018.
\bibitem{Lee} K. Lee et al., “A simple unified framework for detecting out-of-distribution samples and adversarial attacks,” in Adv. Neural Infor. Process. Syst. 31st: Ann. Conf. Neural Infor. Process. Syst., 2018, pp. 7167–7177.

\bibitem{Ribeiro} M. T. Ribeiro, S. Singh, and C. Guestrin, “” why should i trust you?” explaining the predictions of any classifier,” in Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 1135–1144.
\bibitem{Shrikumar} A. Shrikumar, P. Greenside, and A. Kundaje, “Learning important features through propagating activation differences,” in International conference on machine learning. PMLR, 2017, pp. 3145–3153.
\bibitem{LundbergandS} S.M.LundbergandS.-I.Lee,“Aunifiedapproachtointerpretingmodel predictions,” Advances in neural information processing systems, vol. 30, 2017.

\bibitem{ZhangK}Zhang, K., Xu, P. and Zhang, J., 2020, October. Explainable AI in deep reinforcement learning models: A shap method applied in power system emergency control. In 2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2) (pp. 711-716). IEEE.


% \bibitem{Singh} A. Singh, S. Sengupta, and V. Lakshminarayanan, “Explainable deep learning models in medical image analysis,” Journal of Imaging, vol. 6, no. 6, p. 52, 2020.

% \bibitem{Mangalathu} S. Mangalathu, S.-H. Hwang, and J.-S. Jeon, “Failure mode and effects analysis of rc members based on machine-learning-based shapley addi- tive explanations (shap) approach,” Engineering Structures, vol. 219, p. 110927, 2020.

% \bibitem{Aouf}L. He, N. Aouf, and B. Song, “Explainable deep reinforcement learning for uav autonomous path planning,” Aerospace science and technology, vol. 118, p. 107052, 2021.
% \bibitem{Parsa} A. B. Parsa, A. Movahedi, H. Taghipour, S. Derrible, and A. K. Mohammadian, “Toward safer highways, application of xgboost and shap for real-time accident detection and feature analysis,” Accident Analysis \& Prevention, vol. 136, p. 105405, 2020.
% \bibitem{Huang}  H. Wu, A. Huang, and J. W. Sutherland, “Layer-wise relevance prop-
% agation for interpreting lstm-rnn decisions in predictive maintenance,” The International Journal of Advanced Manufacturing Technology, vol. 118, no. 3, pp. 963, 2022.

% \bibitem{Warnecke} A. Warnecke, D. Arp, C. Wressnegger, and K. Rieck, “Evaluating explanation methods for deep learning in security,” in 2020 IEEE european symposium on security and privacy (EuroS\&P). IEEE, 2020, pp. 158.

    \bibitem{McInnes.}
	L. McInnes, J. Healy, and J. Melville, “UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,” ArXiv e- prints, Feb. 2018.
  
 
\end{thebibliography}
% Indicate the start of the appendix
\appendix

% Change the title for the following sections to Appendix if needed
\renewcommand{\thesection}{Appendix \Alph{section}}

% \section{Figures}
% % ... (appendix content)

\subsection{Additional Figures}

        
% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{paper_images/correct absolute threshold.png}
%     \caption{correct absolute threshold}
%     \label{fig:fgsm_c}
% \end{figure*}
% \par\medskip % Adds space between the two subfigures
% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{paper_images/correct negatice threshold.png}
%     \caption{correct negatice threshold}
%     \label{fig:deep_c}
% \end{figure*}
% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{paper_images/correct positive threshold.png}
%     \caption{correct positive threshold}
%     \label{fig:fgsm}
% \end{figure*}




% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{paper_images/aadversarial absolute threshold.png}
%     \caption{absolute adversarial threshold}
%     \label{fig:deep}
% \end{figure*}
% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{paper_images/negative adversarial threshold.png}
%     \caption{negative adversarial threshold}
%     \label{fig:deep}
% \end{figure*}
% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{paper_images/positve adversarial threshold.png}
%     \caption{positve adversarial threshold}
%     \label{fig:deep}
% \end{figure*}

\end{document}
